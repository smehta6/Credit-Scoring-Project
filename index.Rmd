--- 
title: "Credit Scoring Project"
author: "Soham Mehta"
date: "03/23/23`"
site: bookdown::bookdown_site
---

# Proposal
For this project, I will be using the South German Credit Data set from the UC Irvine Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29). This data set contains information about 1000 individuals from 1973-1975 who were clients of a large bank in Southern Germany with an equal number of rural and urban branches. The variable of interest (the response variable) is credit risk (binary: good or bad), while there are 20 explanatory variables: status (of the applicants' checking account), credit duration, credit history, Credit Amount, savings, employment duration, installment rate, sex, guarantors (y or n), length of time in current residence, property (has valuable collateral or not), age, installment plan, type of housing, number of credits, number of dependents, landline (y or n), and foreign worker status.Of these, 13 variables are categorical, while 7 are quantitative.The levels of each categorical variable are discussed more in-depth here: http://www1.beuth-hochschule.de/FB_II/reports/Report-2019-004.pdf. This data set is quite clean as there are no N/A values.  

The reason I chose this data set is because I am very interested in the recent findings that credit scores in the US are dramatically clustered by region (https://www.washingtonpost.com/business/2023/02/17/bad-southern-credit-scores/). Several think tanks and news outlets have offered explanations for this phenomenon, ranging from clustering of individuals with similar demographic profiles or the prevalence of certain kinds of debts in specific regions. While I couldn't find any current or even American data, I think that experimenting with different machine learning methods to figure out which factors are considered in what way to most successfully predict credit risk may lend some clarity on which factors might be the root of systematic differences in credit access, even if this isn't an analogous dataset. Therefore, the modeling goal is to find out which creditors have good or bad ratings based on the 20 explanatory variables.

I plan to use these methods: Decision Trees, Random Forest, Naive Bayes, K-nearest neighbors. I thought of using KNN because my variable of interest clearly delineates two groups (good credit risk or bad credit risk). Given how interpretable KNN is, I would be able to clearly see how my observations cluster into "good" and "bad" groups based on different variables. It would be interest to be able to assess how successful this very, in my opinion, intuitive classification method does. I also picked Decision Trees for a similar reason: they provide a really interpretable way to go about identifying the most important variables for predicting my target variable. Furthermore, if I use a one rule approach, I can assess how successful the strategy of using solely using the single most impactful factor of an individual's credit risk classification is.I also want to use Random Forest because it can provide estimates of feature importance, which can help identify the most relevant variables for predicting credit risk, and I ca understand which factors are most strongly associated with good or bad credit ratings if companies use RF. Lastly, I want to use Naive Bayes as I think this is a likely way companies might actually go about this classification task given how computationally efficient and easy it is to implement compared to the other two methods. Also, Naive Bayes is good at dealing with irrelevant factors as it assumes that features are conditionally indepedent given the class label. 

